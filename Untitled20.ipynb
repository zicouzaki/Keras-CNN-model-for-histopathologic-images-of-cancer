{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled20.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YItQDJPoIvPl"
      },
      "source": [
        "#import of the packages\n",
        "\n",
        "from glob import glob \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras,cv2,os\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers import Conv2D, MaxPool2D\n",
        "\n",
        "from tqdm import tqdm_notebook,trange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sBI1B8DI62p"
      },
      "source": [
        "\n",
        "# the path for the data\n",
        "path = \"../input/\" \n",
        "train_path = path + 'train/'\n",
        "test_path = path + 'test/'\n",
        "# train label data path\n",
        "full_train_df = pd.read_csv(\"../input/train_labels.csv\")\n",
        "\n",
        "# load the filenames of all images in the dataset \n",
        "df = pd.DataFrame({'path': glob(os.path.join(train_path,'*.tif'))}) \n",
        "\n",
        "# collection of the file names\n",
        "df['id'] = df.path.map(lambda x: x.split('/')[3].split(\".\")[0]) \n",
        "# read the labels in the csv file\n",
        "labels = pd.read_csv(path+\"train_labels.csv\") \n",
        "# merging labels and filepaths\n",
        "df = df.merge(labels, on = \"id\") \n",
        "\n",
        "def load_data(N,df):\n",
        "    # functions allowing the load of N images using the data df generated \n",
        "    # allocate a numpy array for the images (N, 96x96px, 3 channels, values 0 - 255)\n",
        "    X = np.zeros([N,96,96,3],dtype=np.uint8) \n",
        "    # convert the labels to a numpy array too\n",
        "    y = np.squeeze(df.as_matrix(columns=['label']))[0:N]\n",
        "    # read images one by one, tdqm notebook displays a progress bar\n",
        "    for i, row in tqdm_notebook(df.iterrows(), total=N):\n",
        "        if i == N:\n",
        "            break\n",
        "        X[i] = cv2.imread(row['path'])\n",
        "          \n",
        "    return X,y\n",
        "\n",
        "df.shape\n",
        "\n",
        "# Load N images for exploring data \n",
        "N=5000\n",
        "X,y = load_data(N=N,df=df) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjP1p0iGKqO4"
      },
      "source": [
        "# Number of samples in each class to train on the model \n",
        "SAMPLE_SIZE = 80000\n",
        "\n",
        "# Data paths\n",
        "train_path = '../input/train/'\n",
        "test_path = '../input/test/'\n",
        "\n",
        "# Use 80000 positive and negative examples\n",
        "df_negatives = full_train_df[full_train_df['label'] == 0].sample(SAMPLE_SIZE, random_state=42)\n",
        "df_positives = full_train_df[full_train_df['label'] == 1].sample(SAMPLE_SIZE, random_state=42)\n",
        "\n",
        "# Concatenate the two dfs and shuffle them up\n",
        "train_df = sklearn.utils.shuffle(pd.concat([df_positives, df_negatives], axis=0).reset_index(drop=True))\n",
        "\n",
        "train_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pagEPoyZKw0F"
      },
      "source": [
        "# Custom class for datasets\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, df_data, data_dir = './', transform=None):\n",
        "        super().__init__()\n",
        "        self.df = df_data.values\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img_name,label = self.df[index]\n",
        "        img_path = os.path.join(self.data_dir, img_name+'.tif')\n",
        "        image = cv2.imread(img_path)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYk4mfQWKzaq"
      },
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(p=0.4),\n",
        "    transforms.RandomVerticalFlip(p=0.4),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_data = CreateDataset(df_data=train_df, data_dir=train_path, transform=transforms_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S5OXUznK1oR"
      },
      "source": [
        "# Set Batch Size\n",
        "batch_size = 128\n",
        "\n",
        "# Percentage of training set to use as validation\n",
        "valid_size = 0.25\n",
        "\n",
        "# Training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# Create Samplers\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# Prepare data loaders (combine dataset and sampler)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ7KMlZBK3wm"
      },
      "source": [
        "transforms_test = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Creating test data\n",
        "sample_sub = pd.read_csv(\"../input/sample_test.csv\")\n",
        "test_data = CreateDataset(df_data=sample_sub, data_dir=test_path, transform=transforms_test)\n",
        "\n",
        "# prepare the test loader\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_U-ESXaK6Np"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN,self).__init__()\n",
        "        # Convolutional and Pooling Layers\n",
        "        self.conv1=nn.Sequential(\n",
        "                nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=0),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2,2))\n",
        "        self.conv2=nn.Sequential(\n",
        "                nn.Conv2d(in_channels=32,out_channels=64,kernel_size=2,stride=1,padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2,2))\n",
        "        self.conv3=nn.Sequential(\n",
        "                nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2,2))\n",
        "        self.conv4=nn.Sequential(\n",
        "                nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2,2))\n",
        "        self.conv5=nn.Sequential(\n",
        "                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2,2))\n",
        "        \n",
        "        self.dropout2d = nn.Dropout2d()\n",
        "        \n",
        "        \n",
        "        self.fc=nn.Sequential(\n",
        "                nn.Linear(512*3*3,1024),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(0.4),\n",
        "                nn.Linear(1024,512),\n",
        "                nn.Dropout(0.4),\n",
        "                nn.Linear(512, 1),\n",
        "                nn.Sigmoid())\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \"\"\"Method for Forward Prop\"\"\"\n",
        "        x=self.conv1(x)\n",
        "        x=self.conv2(x)\n",
        "        x=self.conv3(x)\n",
        "        x=self.conv4(x)\n",
        "        x=self.conv5(x)\n",
        "        x=x.view(x.shape[0],-1)\n",
        "        x=self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEbpyloULANK"
      },
      "source": [
        "# Create a complete CNN\n",
        "model = CNN()\n",
        "print(model)\n",
        "\n",
        "# Trainable Parameters\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Number of trainable parameters: \\n{}\".format(pytorch_total_params))\n",
        "\n",
        "# Specify loss function (categorical cross-entropy loss)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00015)\n",
        "\n",
        "# Number of epochs to train the model\n",
        "n_epochs = 20\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "# Keeping track of losses\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "val_auc = []\n",
        "test_accuracies = []\n",
        "valid_accuracies = []\n",
        "auc_epoch = []\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # Keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    \n",
        "    # training \n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # Move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda().float()\n",
        "        target = target.view(-1, 1)\n",
        "        # Clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # Calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # Perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # Update Train loss and accuracies\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "       \n",
        "    # validation \n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # Move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda().float()\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        target = target.view(-1, 1)\n",
        "        output = model(data)\n",
        "        # Calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # Update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        # Output = output.topk()\n",
        "        y_actual = target.data.cpu().numpy()\n",
        "        y_pred = output[:,-1].detach().cpu().numpy()\n",
        "        val_auc.append(roc_auc_score(y_actual, y_pred))        \n",
        "    \n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "    valid_auc = np.mean(val_auc)\n",
        "    auc_epoch.append(np.mean(val_auc))\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "        \n",
        "    # Print training/validation statistics \n",
        "    print('Epoch: {} | Training Loss: {:.6f} | Validation Loss: {:.6f} | Validation AUC: {:.4f}'.format(\n",
        "        epoch, train_loss, valid_loss, valid_auc))\n",
        "    \n",
        "    \n",
        "    # Early stop \n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggcPewhZLSg_"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(valid_losses, label='Validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(frameon=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_1o1rQoLTU4"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "plt.plot(auc_epoch, label='Validation AUC/Epochs')\n",
        "plt.legend(\"\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Area Under the Curve\")\n",
        "plt.legend(frameon=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JITgB_ZHLVBX"
      },
      "source": [
        "# Load best parameters learned from training into our model to make predictions later\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "def imshow(img):\n",
        "    '''Helper function to un-normalize and display an image'''\n",
        "    # Unnormalize\n",
        "    img = img / 2 + 0.5\n",
        "    # Convert from Tensor image and display\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "\n",
        "# Obtain one batch of training images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Convert images to numpy for display\n",
        "images = images.numpy() \n",
        "\n",
        "# Plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# Display 10 images\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    prob = \"Cancer\" if(sample_sub.label[idx] >= 0.5) else \"Normal\" \n",
        "    ax.set_title('{}'.format(prob))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}